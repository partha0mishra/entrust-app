# LoRA Fine-tuning Requirements
torch>=2.0.0
transformers>=4.35.0
peft>=0.7.0
bitsandbytes>=0.41.0
accelerate>=0.24.0
datasets>=2.14.0
scipy>=1.11.0
sentencepiece>=0.1.99
protobuf>=3.20.0
tqdm>=4.65.0

# For GGUF conversion
gguf>=0.1.0
numpy>=1.24.0

# Utilities
pandas>=2.0.0
scikit-learn>=1.3.0
